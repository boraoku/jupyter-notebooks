{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4afe343e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "class Value:\n",
    "    \n",
    "    def __init__(self, data, _children=(), _op='', label=''):\n",
    "        self.data = data\n",
    "        self.grad = 0.0\n",
    "        self._backward = lambda: None #backpropagation, by default nothing to do\n",
    "        self._prev = set(_children) #record the children\n",
    "        self._op = _op\n",
    "        self.label = label\n",
    "    \n",
    "    #repr makes the printed values readable\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data})\"\n",
    "    \n",
    "    \n",
    "    #use double score in phyton to change default operators\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other) #this enables inline operation on Value like a+1\n",
    "        out = Value(self.data + other.data, (self, other), '+')\n",
    "\n",
    "        def _backward():\n",
    "            # = instead of += below in original implementation\n",
    "            self.grad += 1.0 * out.grad  #transfer grad + bugfix accumulate due to multivariable case of chain rule\n",
    "            other.grad += 1.0 * out.grad \n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def __radd__(self, other): #this is for reserve order mult like 2+a, since above only works a.__add__(2) but not in reserve\n",
    "        return self * other\n",
    "    \n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other) #this enables inline operation on Value like a*2\n",
    "        out = Value(self.data * other.data, (self, other), '*')\n",
    "        \n",
    "        def _backward():\n",
    "            # = instead of += below in original implementation\n",
    "            self.grad += other.data * out.grad #chainrule + bugfix: accumulate due to multivariable case of chain rule\n",
    "            other.grad += self.data * out.grad \n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def __rmul__(self, other): #this is for reserve order mult like 2*a, since above only works a.__mul__(2) but not in reserve\n",
    "        return self * other\n",
    "    \n",
    "    \n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, (int, float)), \"only supporting int/float powers\"\n",
    "        out = Value(self.data**other, (self,), f'**{other}')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += other * (self.data**(other-1)) * out.grad #derivate of x**n is n * x**(n-1)\n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    \n",
    "    def __truediv__(self, other):\n",
    "        return self * other**-1\n",
    "    \n",
    "    \n",
    "    def __neg__(self):\n",
    "        return self * -1\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        return self + (-other)\n",
    "    \n",
    "                    \n",
    "    def tanh(self):\n",
    "        x = self.data\n",
    "        t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)\n",
    "        out = Value(t, (self, ), 'tanh')\n",
    "        \n",
    "        def _backward():\n",
    "            # = instead of += below in original implementation\n",
    "            self.grad += (1 - t**2) * out.grad #local derivate of tanh times out.grad due to chain rule + bugfix: accumulate due to multivariable case of chain rule\n",
    "        out._backward = _backward\n",
    "                \n",
    "        return out\n",
    "    \n",
    "                    \n",
    "    def exp(self):\n",
    "        x = self.data\n",
    "        out = Value(math.exp(x), (self ,), 'exp')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += out.data * out.grad # because d(e**x)/dx = e**x * ln_e = e**x since ln_e=1\n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "                    \n",
    "    \n",
    "    def backward(self):\n",
    "\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v): #build topo from the top object down to its all children, start adding the bottom most child\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "        \n",
    "        self.grad = 1.0 #required because its initilized as 0, but is 1.0 for topmost node\n",
    "        for node in reversed(topo): #iterating in topo in reserve order, since top node shall the start point for backpropagation\n",
    "            node._backward()\n",
    "            \n",
    "class Neuron:\n",
    "    \n",
    "    def __init__(self, nin):\n",
    "        self.w = [Value(random.uniform(-1,1)) for _ in range(nin)] #weights\n",
    "        self.b = Value(random.uniform(-1,1)) #bias\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        # w * x +b\n",
    "        # print(list(zip(self.w, x))) # iteration over tuples\n",
    "        act = sum((wi*xi for wi, xi in zip(self.w, x)), self.b) #activation\n",
    "        out = act.tanh()\n",
    "        return out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return self.w + [self.b]\n",
    "\n",
    "class Layer:\n",
    "    \n",
    "    def __init__(self, nin, nout):\n",
    "        self.neurons = [Neuron(nin) for _ in range(nout)]\n",
    "    \n",
    "    def __call__(self,x):\n",
    "        outs = [n(x) for n in self.neurons]\n",
    "        return outs[0] if len(outs) == 1 else outs #return single element if only one\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [p for neuron in self.neurons for p in neuron.parameters()]\n",
    "        #longer form of above one liner\n",
    "        #params = []\n",
    "        #for neuron in self.neurons:\n",
    "        #    ps = neuron.parameters()\n",
    "        #    params.extend(ps)\n",
    "        #return params\n",
    "        \n",
    "class MLP: #Multi Layer Perceptrons\n",
    "    \n",
    "    def __init__(self, nin, nouts):\n",
    "        sz = [nin] + nouts\n",
    "        self.layers = [Layer(sz[i],sz[i+1]) for i in range(len(nouts))]\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "908bfa9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.2974447086810337\n",
      "1 0.2544127965070754\n",
      "2 0.24337035213709007\n",
      "3 0.23345580094146895\n",
      "4 0.22363195874042338\n",
      "5 0.2138050455783443\n",
      "6 0.20404987502336297\n",
      "7 0.1944800992735968\n",
      "8 0.1852079545955001\n",
      "9 0.1763251695460982\n",
      "10 0.16789308279536297\n",
      "11 0.15993979674990777\n",
      "12 0.15246324738119185\n",
      "13 0.145438289571634\n",
      "14 0.1388253538001043\n",
      "15 0.13257850835274124\n",
      "16 0.1266516332559436\n",
      "17 0.12100235994346975\n",
      "18 0.1155940785606103\n",
      "19 0.11039657642718365\n",
      "20 0.10538585600368952\n",
      "21 0.10054354226275125\n",
      "22 0.09585613634758436\n",
      "23 0.09131425347588568\n",
      "24 0.08691190685384104\n",
      "25 0.08264585733999552\n",
      "26 0.07851502890188791\n",
      "27 0.07451998297500269\n",
      "28 0.07066244436277189\n",
      "29 0.06694487359464475\n",
      "30 0.06337008364176558\n",
      "31 0.05994090150550646\n",
      "32 0.05665987692433154\n",
      "33 0.05352904109791225\n",
      "34 0.05054971792681508\n",
      "35 0.047722388985216024\n",
      "36 0.04504661155945881\n",
      "37 0.04252098692708631\n",
      "38 0.04014317395137293\n",
      "39 0.037909941311065246\n",
      "40 0.03581725047618203\n",
      "41 0.03386036097792504\n",
      "42 0.03203394960179459\n",
      "43 0.030332235771652644\n",
      "44 0.028749106447185514\n",
      "45 0.027278235162893204\n",
      "46 0.025913191233131155\n",
      "47 0.024647536499936156\n",
      "48 0.023474908209967758\n",
      "49 0.022389087614504033\n",
      "50 0.02138405466782498\n",
      "51 0.020454029757909917\n",
      "52 0.019593503761378608\n",
      "53 0.01879725790474813\n",
      "54 0.01806037497283804\n",
      "55 0.01737824336796655\n",
      "56 0.016746555421822254\n",
      "57 0.016161301221490124\n",
      "58 0.015618759052154253\n",
      "59 0.015115483396163169\n",
      "60 0.01464829127142893\n",
      "61 0.014214247547760295\n",
      "62 0.01381064975107681\n",
      "63 0.013435012753794518\n",
      "64 0.013085053654901783\n",
      "65 0.012758677074343595\n",
      "66 0.012453961021791492\n",
      "67 0.01216914344795218\n",
      "68 0.011902609545481101\n",
      "69 0.011652879834606335\n",
      "70 0.011418599044174025\n",
      "71 0.011198525780611045\n",
      "72 0.010991522964061876\n",
      "73 0.010796549001681312\n",
      "74 0.010612649661893325\n",
      "75 0.010438950609667976\n",
      "76 0.010274650560942188\n",
      "77 0.0101190150137641\n",
      "78 0.009971370514204585\n",
      "Target : 0.25  | Guess : 0.3051086027779021\n",
      "Target : 0.4  | Guess : 0.4054475317480962\n",
      "Target : 0.75  | Guess : 0.7707095769229715\n",
      "Target : 1.0  | Guess : 0.9024663733078196\n",
      "Solved with Neural Network with 33 no. parameters with loss 0.009971370514204585\n"
     ]
    }
   ],
   "source": [
    "# reinit input and the neural net\n",
    "xs = [[1.1], [1.5], [3.0], [6.0]] #inputs\n",
    "ys = [0.25, 0.40, 0.75, 1.0] #desired targets\n",
    "n = MLP(1, [4,4,1]) #neural net\n",
    "\n",
    "# putting back and forward passes together in a training loop\n",
    "# this is called a gradient descent\n",
    "for k in range(100000):\n",
    "\n",
    "    #forward pass\n",
    "    ydred = [n(x) for x in xs]\n",
    "    loss = sum([(yout - ygt)**2 for ygt, yout in zip(ys, ydred)])\n",
    "    \n",
    "    #zerograd\n",
    "    for p in n.parameters():\n",
    "        p.grad = 0.0\n",
    "    \n",
    "    #backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    #update\n",
    "    for p in n.parameters():\n",
    "        p.data += -0.05 * p.grad\n",
    "        \n",
    "    print(k, loss.data)\n",
    "    \n",
    "    if loss.data < 10e-3:\n",
    "        break    # break here when sufficient precision is achieved\n",
    "\n",
    "# print results\n",
    "for i in range(0, len(ydred)):\n",
    "    print(\"Target :\", ys[i], \" | Guess :\", ydred[i].data)\n",
    "print(\"Solved with Neural Network with\", len(n.parameters()), \"no. parameters with loss\", loss.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fdffa38f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=0.5531896434556004)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n([2.0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
