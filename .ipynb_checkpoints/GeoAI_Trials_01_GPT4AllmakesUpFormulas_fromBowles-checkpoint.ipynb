{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b3393a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using GPT4All as referenced at https://gist.github.com/psychemedia/51f45fbfe160f78605bdd0c1b404e499\n",
    "\"\"\" !!! REMOVE QUOTES TO INSTALL ALL REQUIRED PACKAGES\n",
    "# install llama-cpp and other modules if not installed before\n",
    "!pip install llama-cpp-python\n",
    "!pip install langchain\n",
    "!pip install PyPDF2\n",
    "!pip install faiss-cpu\n",
    "!pip install tiktoken\n",
    "!pip install pygpt4all\n",
    "!pip install pyllamacpp\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9d8859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF -> TXT from Geotechnical Classic: Foundation Analysis and Design Joseph E Bowles\n",
    "\"\"\" !!! DONE ALREADY AND USING ONLY A SECTION FROM THE BOOK\n",
    "import os\n",
    "import io\n",
    "from PyPDF2 import PdfReader\n",
    "# read pdf\n",
    "reader = PdfReader('FoundationAnalysisandDesignJosephEBowles.pdf')\n",
    "\n",
    "# read data from the file and put them into a variable called raw_text\n",
    "raw_text = ''\n",
    "for i, page in enumerate(reader.pages):\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        raw_text += text\n",
    "\n",
    "# Create a txt file\n",
    "text_data = './text_data.txt'\n",
    "with open(text_data, 'w', encoding=\"utf-8\") as fo:\n",
    "    fo.write(raw_text)\n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1b02d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from ./models/gpt4all-lora-quantized.bin\n",
      "llama.cpp: can't use mmap because tensors are not aligned; convert to new format to avoid this\n",
      "llama_model_load_internal: format     = ggmf v1 (old version with no mmap support)\n",
      "llama_model_load_internal: n_vocab    = 32001\n",
      "llama_model_load_internal: n_ctx      = 2048\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size = 4113744.11 KB\n",
      "llama_model_load_internal: mem required  = 5809.33 MB (+ 2052.00 MB per state)\n",
      "...................................................................................................\n",
      ".\n",
      "llama_init_from_file: kv self size  = 2048.00 MB\n",
      "AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import LlamaCppEmbeddings\n",
    "embeddings = LlamaCppEmbeddings(model_path='./models/gpt4all-lora-quantized.bin', n_ctx=2048)\n",
    "#embeddings = LlamaCppEmbeddings(model_path='./models/ggml-model-q4_0.bin') # Facebook LLaMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b12fff70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "with open(\"./GeoAI_Trials_01_text.txt.txt\") as f: #Section 2-14 from the book\n",
    "    text_data_elastic = f.read()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_text(text_data_elastic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "444a9bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "persist_directory = 'db01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a0c2945",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB with persistence: data will be stored in: db01\n",
      "\n",
      "llama_print_timings:        load time =  1619.81 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 14021.85 ms /   307 tokens (   45.67 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 14024.71 ms\n",
      "\n",
      "llama_print_timings:        load time =  1619.81 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 11967.68 ms /   290 tokens (   41.27 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 11970.34 ms\n",
      "\n",
      "llama_print_timings:        load time =  1619.81 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 14804.51 ms /   360 tokens (   41.12 ms per token)\n",
      "llama_print_timings:        eval time =    63.75 ms /     1 runs   (   63.75 ms per run)\n",
      "llama_print_timings:       total time = 14870.83 ms\n",
      "\n",
      "llama_print_timings:        load time =  1619.81 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 15509.16 ms /   378 tokens (   41.03 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 15511.61 ms\n",
      "\n",
      "llama_print_timings:        load time =  1619.81 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 13591.15 ms /   333 tokens (   40.81 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 13593.44 ms\n",
      "\n",
      "llama_print_timings:        load time =  1619.81 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 11910.99 ms /   293 tokens (   40.65 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 11913.00 ms\n",
      "\n",
      "llama_print_timings:        load time =  1619.81 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 11622.50 ms /   287 tokens (   40.50 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 11624.52 ms\n",
      "\n",
      "llama_print_timings:        load time =  1619.81 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 10892.36 ms /   269 tokens (   40.49 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 10894.22 ms\n",
      "\n",
      "llama_print_timings:        load time =  1619.81 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 13277.62 ms /   324 tokens (   40.98 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 13279.95 ms\n",
      "\n",
      "llama_print_timings:        load time =  1619.81 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 13588.08 ms /   333 tokens (   40.81 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 13590.31 ms\n",
      "\n",
      "llama_print_timings:        load time =  1619.81 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 13766.01 ms /   335 tokens (   41.09 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 13768.38 ms\n",
      "\n",
      "llama_print_timings:        load time =  1619.81 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  9637.78 ms /   237 tokens (   40.67 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  9639.90 ms\n",
      "\n",
      "llama_print_timings:        load time =  1619.81 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 10195.79 ms /   250 tokens (   40.78 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 10197.69 ms\n",
      "\n",
      "llama_print_timings:        load time =  1619.81 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 10718.06 ms /   264 tokens (   40.60 ms per token)\n",
      "llama_print_timings:        eval time =    60.45 ms /     1 runs   (   60.45 ms per run)\n",
      "llama_print_timings:       total time = 10780.48 ms\n",
      "\n",
      "llama_print_timings:        load time =  1619.81 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 12180.83 ms /   300 tokens (   40.60 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 12182.92 ms\n",
      "\n",
      "llama_print_timings:        load time =  1619.81 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 12101.11 ms /   291 tokens (   41.58 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 12103.28 ms\n",
      "\n",
      "llama_print_timings:        load time =  1619.81 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 11243.90 ms /   277 tokens (   40.59 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 11245.85 ms\n"
     ]
    }
   ],
   "source": [
    "# create database\n",
    "docsearch = Chroma.from_texts(texts, embeddings, persist_directory=persist_directory)\n",
    "docsearch.persist()\n",
    "# or load database\n",
    "#docsearch = Chroma(persist_directory=persist_directory, embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cebf638",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from ./models/gpt4all-lora-quantized.bin\n",
      "llama.cpp: can't use mmap because tensors are not aligned; convert to new format to avoid this\n",
      "llama_model_load_internal: format     = ggmf v1 (old version with no mmap support)\n",
      "llama_model_load_internal: n_vocab    = 32001\n",
      "llama_model_load_internal: n_ctx      = 2048\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size = 4113744.11 KB\n",
      "llama_model_load_internal: mem required  = 5809.33 MB (+ 2052.00 MB per state)\n",
      "...................................................................................................\n",
      ".\n",
      "llama_init_from_file: kv self size  = 2048.00 MB\n",
      "AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "llm = LlamaCpp(model_path='./models/gpt4all-lora-quantized.bin', n_ctx=2048)\n",
    "#llm = LlamaCpp(model_path='./models/ggml-model-q4_0.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4756c83e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create chain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "chain = load_qa_chain(llm, chain_type=\"stuff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b6dc862",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  1619.81 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  3319.96 ms /    16 tokens (  207.50 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  3321.72 ms\n",
      "\n",
      "llama_print_timings:        load time =  2029.93 ms\n",
      "llama_print_timings:      sample time =    55.43 ms /    57 runs   (    0.97 ms per run)\n",
      "llama_print_timings: prompt eval time = 58812.95 ms /   372 tokens (  158.10 ms per token)\n",
      "llama_print_timings:        eval time = 11204.88 ms /    56 runs   (  200.09 ms per run)\n",
      "llama_print_timings:       total time = 70099.19 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' The typical Elasticity Modulus values for hard clay are in the range of (200 to 500) X su (2-68) to (1200 to 2000) X su (2-70).'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# query\n",
    "#query = \"What is the value range for static stress-strain modulus for hard clay?\"\n",
    "query = \"What is the typical Elasticity Modulus values for hard clay?\"\n",
    "docs = docsearch.similarity_search(query)\n",
    "chain.run(input_documents=docs, question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7059b2dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"in more detail in Chap. 20) may be from two to ten times the static value.\\nBoth Es and Poisson's ratio /JL are heavily dependent on the following:\\n1. Method of laboratory test (confined, unconfined, undrained, drained).\", metadata={'source': './text_data_elastic.txt'}),\n",
       " Document(page_content=\"in more detail in Chap. 20) may be from two to ten times the static value.\\nBoth Es and Poisson's ratio /JL are heavily dependent on the following:\\n1. Method of laboratory test (confined, unconfined, undrained, drained).\", metadata={'source': './text_data_elastic.txt'}),\n",
       " Document(page_content=\"in more detail in Chap. 20) may be from two to ten times the static value.\\nBoth Es and Poisson's ratio /JL are heavily dependent on the following:\\n1. Method of laboratory test (confined, unconfined, undrained, drained).\", metadata={'source': './text_data_elastic.txt'}),\n",
       " Document(page_content='Normally consolidated sensitive clay:\\nEs = (200 to 500) X su (2-68)\\nNormally consolidated insensitive and lightly overconsolidated clay:\\nEs = (750 to 1200) X su (2-69)\\nHeavily overconsolidated clay\\nEs = (1500 to 2000) X su (2-70)', metadata={'source': './text_data_elastic.txt'})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
