{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d973baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Boiler Plate\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#select one of the devices, note GPU (Nvidia or Metal) slower than CPU atm\n",
    "\n",
    "#cpu\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "#nvidia cuda\n",
    "#device = torch.device(\"cuda\")\n",
    "\n",
    "#high-performance training on Metal GPU for Mac - https://pytorch.org/docs/stable/notes/mps.html\n",
    "#device = torch.device(\"mps\")\n",
    "#%env PYTORCH_ENABLE_MPS_FALLBACK=0\n",
    "\n",
    "#words(names) loaded\n",
    "words = open('AI4-names.txt', 'r').read().splitlines()\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)} \n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s, i in stoi.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92eb1fe",
   "metadata": {},
   "source": [
    "# Previously Achieved Results\n",
    "\n",
    "## BORA'S FAST NN SETUP\n",
    "Below setup with `17735`no. parameters, results in Losses of ~`1.9850` for training / ~`2.0858` for dev/validation data sets:\n",
    "\n",
    "`block_size = 4 | d = 4 | hl = 400 | mb_size = 64 | lr=[0.2, 0.1, 0.05, 0.01] | loop_lr = 200000`\n",
    ">`block_size` is the context length; how many characters do we take to predict the next one),<br>\n",
    ">`d` is the embedding size,<br>\n",
    ">`hl` is number of hidden layers,<br>\n",
    ">`mb_size` is minibatch size,<br>\n",
    ">`lr` is learning rates decaying in four parts at the halfway of each loop (two total),<br>\n",
    ">`loop_lr` is number steps in each loop for gradient descent, applied twice (two loops).\n",
    "\n",
    "Generated words with Uniqueness Score of **~95%** and other stats as follows:\n",
    "\n",
    "| First Run| Second Run| Third Run |\n",
    "|---|---|---|\n",
    "| xevan, snylie, olden, sandanerimya, trus, renulia, judiya, madhaiser, edin+, roon, kelie, bradashiyan, suka, carriusiah, sanny, brixna, delviha, krith, kaeliyah, kinz | vick, aswynnie, fonyn, maramine, chanory, iuganor, kobia, alasimyurdei, kipelyn, elaysen, elezarias, jedy, annis, wafley, briyona, thaidonie, bexlee, tashad, srion, keltan | tyla, eikdy, dairo, corron, araiya, alvier, ocland, axina, drenli, malia, kaniya+, akima, koteh, bouro, abdor, katthilanarthli, juony, cai+, aymager, broken |\n",
    "| 1 no. (+ marked) generated words are exact copies from the training set; uniqueness score 19/20, 95%. | 0 no. (+ marked) generated words are exact copies from the training set; uniqueness score 20/20, 100%. | 2 no. (+ marked) generated words are exact copies from the training set; uniqueness score 18/20, 90%. |\n",
    "| Losses @ Train & Dev Sets: 1.9902 & 2.0940 | Losses @ Train & Dev Sets: 1.9883 & 2.0854 | Losses @ Train & Dev Sets: 1.9765 & 2.0779 |\n",
    "\n",
    "<hr/>\n",
    "\n",
    "## BORA'S OPTIMUM NN SETUP\n",
    "Below setup with `26662`no. parameters, results in Losses of ~`1.9115` for training / ~`2.0564` for dev/validation data sets:\n",
    "\n",
    "`block_size = 5 | d = 5 | hl = 500 | mb_size = 80 | lr=[0.2, 0.1, 0.05, 0.01] | loop_lr = 200000`\n",
    ">`block_size` is the context length; how many characters do we take to predict the next one),<br>\n",
    ">`d` is the embedding size,<br>\n",
    ">`hl` is number of hidden layers,<br>\n",
    ">`mb_size` is minibatch size,<br>\n",
    ">`lr` is learning rates decaying in four parts at the halfway of each loop (two total),<br>\n",
    ">`loop_lr` is number steps in each loop for gradient descent, applied twice (two loops).\n",
    "\n",
    "Generated words with Uniqueness Score of **~92%** and other stats as follows:\n",
    "\n",
    "| First Run| Second Run| Third Run |\n",
    "|---|---|---|\n",
    "| tyma, eindy, dairo, corron, aibre, zalanis, oclaun, axiah, drenor, malia, kaniya+, frica, kator, brumi, abdore, jillie, warthli, juones, gicay, agar | angelise, gabrier, bella, zannin, finsly, iuganous, kyrael, derry, addie+, costyn, elays, rhee, zariah+, jedya, anney, palest, aidona, thaison, mebibeh, mirsh | xossa, veyli, soldee, samaaher, myran, breya+, jusniya, kyatt, dechetriel, jerron+, kelse, britang, yuvinna, diang, mairse, manyia, manila, kamot, kristophe, dannga |\n",
    "| 1 no. (+ marked) generated words are exact copies from the training set; uniqueness score 19/20, 95%. | 2 no. (+ marked) generated words are exact copies from the training set; uniqueness score 18/20, 90%. | 2 no. (+ marked) generated words are exact copies from the training set; uniqueness score 18/20, 90%. |\n",
    "| Losses @ Train & Dev Sets: 1.9150 & 2.0533 | Losses @ Train & Dev Sets: 1.9145 & 2.0598 | Losses @ Train & Dev Sets: 1.9050 & 2.0560 |\n",
    "\n",
    "<hr/>\n",
    "\n",
    "## BORA'S BIG NN SETUP\n",
    "Below setup with `64297`no. parameters, results in Losses of ~`2.0337` for training / ~`2.2310` for dev/validation data sets:\n",
    "\n",
    "`block_size = 10 | d = 10 | hl = 500 | mb_size = 100 | lr=[0.2, 0.1, 0.05, 0.01] | loop_lr = 200000`\n",
    ">`block_size` is the context length; how many characters do we take to predict the next one),<br>\n",
    ">`d` is the embedding size,<br>\n",
    ">`hl` is number of hidden layers,<br>\n",
    ">`mb_size` is minibatch size,<br>\n",
    ">`lr` is learning rates decaying in four parts at the halfway of each loop (two total),<br>\n",
    ">`loop_lr` is number steps in each loop for gradient descent, applied twice (two loops).\n",
    "\n",
    "Generated words with Uniqueness Score of **~97%** and other stats as follows:\n",
    "\n",
    "| First Run| Second Run| Third Run |\n",
    "|---|---|---|\n",
    "| xobif, veylin, oleejah, zoshern, marthey, renulyn, fudryan, jahiyah, ferin, roou, kelie, britasgh, anzisa, diano, maira+, fannykan, anilia, dahmand, raylae, danngo | jazaya, tahron, valenia, eyocin, kaniella, madoneia, bezren, bitsiel, anslen, miuft, lulabe, waina, ajaril, tnony, deyson, zeloo, veliah, rankellond, kaisyncl, kaesiyah | jazaya, tahanne, tirihase, kylen, deley, aliah+, aria, bervewve, aureona, syana, nufayla, kayvan, alaija, alien, nyliey, falleegh, vaigah, raykel, tahar, leylalose |\n",
    "| 1 no. (+ marked) generated words are exact copies from the training set; uniqueness score 19/20, 95%. | 0 no. (+ marked) generated words are exact copies from the training set; uniqueness score 20/20, 100%. | 1 no. (+ marked) generated words are exact copies from the training set; uniqueness score 19/20, 95%. |\n",
    "| Losses @ Train & Dev Sets: 2.0386 & 2.2338 | Losses @ Train & Dev Sets: 2.0277 & 2.2251 | Losses @ Train & Dev Sets: 2.0348 & 2.2341 |\n",
    "\n",
    "<hr/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "752abef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters of NN\n",
    "\n",
    "#learning rates same for all NN setups\n",
    "lr=[0.2, 0.1, 0.05, 0.01] #learning rates decaying in four parts, halfway for each loop defined below\n",
    "loop_lr = 200000 #single loop no for learning rates, applied twice\n",
    "\n",
    "#NN parameters to be played with \n",
    "name_parameterSet = \"\"\n",
    "block_size = 1 #context length: how many characters do we take to predict the next one\n",
    "d = 1 #embedding size\n",
    "hl = 10 #no of hidden layers\n",
    "mb_size = 10 #minibatch size\n",
    "\n",
    "#make selection here        \n",
    "selection = \"fast\" #fast, optimum or big\n",
    "\n",
    "#from these predefined sets\n",
    "if selection == \"fast\":\n",
    "    # setup 1\n",
    "    name_parameterSet = \"BORA'S FAST NN SETUP\"\n",
    "    block_size = 4; d = 4; hl = 400; mb_size = 64\n",
    "    \n",
    "elif selection == \"optimum\":\n",
    "    # setup 2\n",
    "    name_parameterSet = \"BORA'S OPTIMUM NN SETUP\"\n",
    "    block_size = 5; d = 5; hl = 500; mb_size = 80\n",
    "    \n",
    "elif selection == \"big\":\n",
    "    # setup 3\n",
    "    name_parameterSet = \"BORA'S BIG NN SETUP\"\n",
    "    block_size = 10; d = 10; hl = 500; mb_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71398c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182580, 4]) torch.Size([182580])\n",
      "torch.Size([22767, 4]) torch.Size([22767])\n",
      "torch.Size([22799, 4]) torch.Size([22799])\n"
     ]
    }
   ],
   "source": [
    "#build dataset\n",
    "def build_dataset(words):\n",
    "    X, Y = [], []\n",
    "    \n",
    "    for w in words:\n",
    "    \n",
    "        #print(w)\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            #print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
    "            context = context[1:] + [ix] #crop and append\n",
    "    \n",
    "    X = torch.tensor(X, device=device)\n",
    "    Y = torch.tensor(Y, device=device)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "# training split, dev/validation split, test split\n",
    "# 80%, 10%, 10%\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])\n",
    "Xte, Yte = build_dataset(words[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81fb0754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected NN parameter set: BORA'S FAST NN SETUP\n",
      "\n",
      "Run #1\n",
      "Progressed 100% at Step 400000 of 400000\n",
      "Final Loss @ MiniBatch 2.0236551761627197\n",
      "\n",
      "Run #2\n",
      "Progressed 100% at Step 400000 of 400000\n",
      "Final Loss @ MiniBatch 2.006887674331665\n",
      "\n",
      "Run #3\n",
      "Progressed 100% at Step 400000 of 400000\n",
      "Final Loss @ MiniBatch 1.7776563167572021\n"
     ]
    }
   ],
   "source": [
    "lossT_all = 0.0\n",
    "lossD_all = 0.0\n",
    "bestquality_all = 0.0\n",
    "bestgenwords_all = []\n",
    "bestcount_all = []\n",
    "loss_all = []\n",
    "\n",
    "print(\"\\nSelected NN parameter set: \" + name_parameterSet)\n",
    "\n",
    "for run in range(3):\n",
    "    print('\\nRun #' + str(run+1))\n",
    "    td = block_size * d \n",
    "    g = torch.Generator().manual_seed(2147483647) #deterministic reproducibility\n",
    "    C = torch.randn((27,d), generator=g).to(device) \n",
    "    W1 = torch.randn((td,hl), generator=g).to(device) \n",
    "    b1 = torch.randn(hl, generator=g).to(device) \n",
    "    W2 = torch.randn((hl,27), generator=g).to(device) \n",
    "    b2 = torch.randn(27, generator=g).to(device) \n",
    "    parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "    for p in parameters:\n",
    "        p.requires_grad = True\n",
    "\n",
    "    lossi = []\n",
    "    stepi = []\n",
    "\n",
    "    pp = 0\n",
    "    for j in range(2):\n",
    "        for i in range(loop_lr):\n",
    "\n",
    "            #minibatch\n",
    "            ix = torch.randint(0, Xtr.shape[0], (mb_size,), device=device) #use training set\n",
    "\n",
    "            #forward pass\n",
    "            emb = C[Xtr[ix]] #use training set\n",
    "            h = torch.tanh(emb.view(-1, td) @ W1 + b1) #up to 30 here too\n",
    "            logits = h @ W2 + b2 \n",
    "            loss = F.cross_entropy(logits, Ytr[ix]) #use training set\n",
    "            #print(loss.item())\n",
    "\n",
    "            #backward pass\n",
    "            for p in parameters:\n",
    "                p.grad = None\n",
    "            loss.backward()\n",
    "\n",
    "            lr_current = 0\n",
    "\n",
    "            if j < 1:\n",
    "                lr_current = lr[0] if i < loop_lr/2 else lr[1] #first run\n",
    "            else:\n",
    "                lr_current = lr[2] if i < loop_lr/2 else lr[3] #second run\n",
    "\n",
    "            for p in parameters:\n",
    "                p.data += -lr_current * p.grad\n",
    "\n",
    "            #print progress\n",
    "            currentStep = i + 1 + loop_lr*j\n",
    "            totalNoSteps = loop_lr * 2\n",
    "            currentProgress = int(float(currentStep) / float(totalNoSteps) *100)\n",
    "            reportingStep = 2\n",
    "            if (currentStep > pp) and (currentProgress % reportingStep == 0):\n",
    "                progress = \"Progressed \" + str(currentProgress) + \"%\" + \" at Step \" + str(currentStep) + \" of \" + str(totalNoSteps)\n",
    "                print(progress, end=\"\\r\")\n",
    "                pp = currentStep + (totalNoSteps * 2 / 100) - reportingStep #cut-off until next step\n",
    "\n",
    "            #track stats\n",
    "            stepi.append(i+loop_lr*j)\n",
    "            lossi.append(loss.log10().item())\n",
    "\n",
    "    print('\\nFinal Loss @ MiniBatch ' + str(loss.item()))\n",
    "    # plt.plot(stepi, lossi)\n",
    "\n",
    "    emb = C[Xtr] #use training set to check\n",
    "    h = torch.tanh(emb.view(-1, td) @ W1 + b1) \n",
    "    logits = h @ W2 + b2\n",
    "    lossT = F.cross_entropy(logits, Ytr) #use training set to check\n",
    "    lossT_all += lossT / 3.0\n",
    "\n",
    "    emb = C[Xdev] #use dev set to evaluate\n",
    "    h = torch.tanh(emb.view(-1, td) @ W1 + b1)\n",
    "    logits = h @ W2 + b2\n",
    "    lossD = F.cross_entropy(logits, Ydev) #use dev set to evaluate\n",
    "    lossD_all += lossD / 3.0\n",
    "\n",
    "    #finally lets sample from the new NN model\n",
    "\n",
    "    #sample 10 time for maximum quality\n",
    "    bestquality = 0.0\n",
    "    bestgenwords = \"\"\n",
    "    bestcountexct = 0\n",
    "\n",
    "    for rand in range(11):\n",
    "        g = torch.Generator().manual_seed(2147483647+rand)\n",
    "\n",
    "        generation = []\n",
    "        numberofgen = 20\n",
    "        for i in range(numberofgen):\n",
    "\n",
    "            out = []\n",
    "            context = [0] * block_size #start with dot\n",
    "            while True:\n",
    "                emb = C[torch.tensor([context])] # (1, block_size, d)\n",
    "                h = torch.tanh(emb.view(1, -1) @ W1 + b1)\n",
    "                logits = h @ W2 + b2\n",
    "                probs = F.softmax(logits, dim=1)\n",
    "                ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "                context = context[1:] + [ix]\n",
    "                out.append(ix)\n",
    "                if ix == 0:\n",
    "                    break\n",
    "\n",
    "            generated = ''.join(itos[i] for i in out).rstrip(\".\")\n",
    "            generation.append(generated) \n",
    "\n",
    "\n",
    "        #find the words which already existed in the training set\n",
    "        genwords = \"\"\n",
    "        countexct = 0\n",
    "        #c = [0 for x in range(0, len(generation))] #just marking the index\n",
    "        for i, x in enumerate(generation):\n",
    "            if len(genwords) > 0:\n",
    "                genwords += \", \"\n",
    "\n",
    "            genwords += x\n",
    "\n",
    "            if x in words[:n1]:\n",
    "                #c[i] = 1\n",
    "                countexct += 1\n",
    "                genwords += \"+\"\n",
    "\n",
    "        quality = (100*(1.0 - float(countexct)/float(numberofgen)))\n",
    "\n",
    "        if quality > bestquality:\n",
    "            bestgenwords = genwords\n",
    "            bestcountexct = countexct\n",
    "            bestquality = quality\n",
    "\n",
    "    bestquality_all += bestquality / 3.0\n",
    "    bestgenwords_all.append(bestgenwords)\n",
    "    bestcount_all.append(str(bestcountexct) + \" no. (+ marked) generated words are exact copies from the training set; uniqueness score \" + str(numberofgen-bestcountexct) + \"/\" + str(numberofgen) + \", \" + f'{bestquality:.0f}' + \"%.\")\n",
    "    loss_all.append(\"Losses @ Train & Dev Sets: \" + f'{lossT.item():.4f}' + \" & \" +  f'{lossD.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "063ef210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## BORA'S FAST NN SETUP\n",
       "Below setup with `17735`no. parameters, results in Losses of ~`1.9773` for training / ~`2.0701` for dev/validation data sets:\n",
       "\n",
       "`block_size = 4 | d = 4 | hl = 400 | mb_size = 64 | lr=[0.2, 0.1, 0.05, 0.01] | loop_lr = 200000`\n",
       ">`block_size` is the context length; how many characters do we take to predict the next one),<br>\n",
       ">`d` is the embedding size,<br>\n",
       ">`hl` is number of hidden layers,<br>\n",
       ">`mb_size` is minibatch size,<br>\n",
       ">`lr` is learning rates decaying in four parts at the halfway of each loop (two total),<br>\n",
       ">`loop_lr` is number steps in each loop for gradient descent, applied twice (two loops).\n",
       "\n",
       "Generated words with Uniqueness Score of **~93%** and other stats as follows:\n",
       "\n",
       "| First Run| Second Run| Third Run |\n",
       "|---|---|---|\n",
       "| garri, yadi, tayn, aleja, ayler, allo, jubel, deppe, dalen, kaizy, kardo, eeng, cosley, reei, abhet, elson+, zolin, braxlyn, kaedence, anell | abilyani, mihe, raydel+, yosmeeya, aadtan, silma, jashaanela, pynzamiroy, idam, braquen, sohaia, maysika, jaseus, karley+, megunnday, hawilker, avysi, oliyah, dishanvika, erianna | victon, wylah, dern, nolana, naka, finos, micka, orson+, falayae, kyurdai, kiphustoe, austocee, zarias, jedy, annistophir, nasiyora, thahdon, melie, houd, shodessa |\n",
       "| 1 no. (+ marked) generated words are exact copies from the training set; uniqueness score 19/20, 95%. | 2 no. (+ marked) generated words are exact copies from the training set; uniqueness score 18/20, 90%. | 1 no. (+ marked) generated words are exact copies from the training set; uniqueness score 19/20, 95%. |\n",
       "| Losses @ Train & Dev Sets: 1.9790 & 2.0685 | Losses @ Train & Dev Sets: 1.9771 & 2.0738 | Losses @ Train & Dev Sets: 1.9756 & 2.0681 |\n",
       "<hr/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Markdown output\n",
    "from IPython.display import display, Markdown, Latex\n",
    "output = \"\"\"\\\n",
    "## {name}\n",
    "Below setup with `{parameters}`no. parameters, results in Losses of ~`{lossT}` for training / ~`{lossD}` for dev/validation data sets:\n",
    "\n",
    "`block_size = {block_size} | d = {d} | hl = {hl} | mb_size = {mb_size} | lr={lr} | loop_lr = {loop_lr}`\n",
    ">`block_size` is the context length; how many characters do we take to predict the next one),<br>\n",
    ">`d` is the embedding size,<br>\n",
    ">`hl` is number of hidden layers,<br>\n",
    ">`mb_size` is minibatch size,<br>\n",
    ">`lr` is learning rates decaying in four parts at the halfway of each loop (two total),<br>\n",
    ">`loop_lr` is number steps in each loop for gradient descent, applied twice (two loops).\n",
    "\n",
    "Generated words with Uniqueness Score of **~{bestquality}%** and other stats as follows:\n",
    "\n",
    "| First Run| Second Run| Third Run |\n",
    "|---|---|---|\n",
    "| {bestgen1} | {bestgen2} | {bestgen3} |\n",
    "| {bestcount1} | {bestcount2} | {bestcount3} |\n",
    "| {loss1} | {loss2} | {loss3} |\n",
    "<hr/>\n",
    "\"\"\".format(\n",
    "    name=name_parameterSet,\n",
    "    parameters=sum(p.nelement() for p in parameters),\n",
    "    lossT=f'{lossT_all:.4f}',\n",
    "    lossD=f'{lossD_all:.4f}',\n",
    "    block_size = block_size, #context length: how many characters do we take to predict the next one\n",
    "    d = d, #embedding size\n",
    "    hl = hl, #no of hidden layers\n",
    "    mb_size = mb_size, #minibatch size\n",
    "    lr= lr,#learning rates decaying in four parts, halfway for each loop defined below\n",
    "    loop_lr = loop_lr, #single loop no for learning rates, applied twice\n",
    "    bestquality = f'{bestquality_all:.0f}',\n",
    "    bestgen1 = bestgenwords_all[0],\n",
    "    bestgen2 = bestgenwords_all[1],\n",
    "    bestgen3 = bestgenwords_all[2],\n",
    "    bestcount1 = bestcount_all[0],\n",
    "    bestcount2 = bestcount_all[1],\n",
    "    bestcount3 = bestcount_all[2],\n",
    "    loss1 = loss_all[0],\n",
    "    loss2 = loss_all[1],\n",
    "    loss3 = loss_all[2]\n",
    ")\n",
    "display(Markdown(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a01642a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Raw Markdown code:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## BORA'S FAST NN SETUP\n",
      "Below setup with `17735`no. parameters, results in Losses of ~`1.9773` for training / ~`2.0701` for dev/validation data sets:\n",
      "\n",
      "`block_size = 4 | d = 4 | hl = 400 | mb_size = 64 | lr=[0.2, 0.1, 0.05, 0.01] | loop_lr = 200000`\n",
      ">`block_size` is the context length; how many characters do we take to predict the next one),<br>\n",
      ">`d` is the embedding size,<br>\n",
      ">`hl` is number of hidden layers,<br>\n",
      ">`mb_size` is minibatch size,<br>\n",
      ">`lr` is learning rates decaying in four parts at the halfway of each loop (two total),<br>\n",
      ">`loop_lr` is number steps in each loop for gradient descent, applied twice (two loops).\n",
      "\n",
      "Generated words with Uniqueness Score of **~93%** and other stats as follows:\n",
      "\n",
      "| First Run| Second Run| Third Run |\n",
      "|---|---|---|\n",
      "| garri, yadi, tayn, aleja, ayler, allo, jubel, deppe, dalen, kaizy, kardo, eeng, cosley, reei, abhet, elson+, zolin, braxlyn, kaedence, anell | abilyani, mihe, raydel+, yosmeeya, aadtan, silma, jashaanela, pynzamiroy, idam, braquen, sohaia, maysika, jaseus, karley+, megunnday, hawilker, avysi, oliyah, dishanvika, erianna | victon, wylah, dern, nolana, naka, finos, micka, orson+, falayae, kyurdai, kiphustoe, austocee, zarias, jedy, annistophir, nasiyora, thahdon, melie, houd, shodessa |\n",
      "| 1 no. (+ marked) generated words are exact copies from the training set; uniqueness score 19/20, 95%. | 2 no. (+ marked) generated words are exact copies from the training set; uniqueness score 18/20, 90%. | 1 no. (+ marked) generated words are exact copies from the training set; uniqueness score 19/20, 95%. |\n",
      "| Losses @ Train & Dev Sets: 1.9790 & 2.0685 | Losses @ Train & Dev Sets: 1.9771 & 2.0738 | Losses @ Train & Dev Sets: 1.9756 & 2.0681 |\n",
      "<hr/>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display(Markdown(\"## Raw Markdown code:\")); print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
